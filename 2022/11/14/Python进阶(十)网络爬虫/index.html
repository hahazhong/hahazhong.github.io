<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="用Python获取网络数据网络数据采集是 Python 语言非常擅长的领域，上节课我们讲到，实现网络数据采集的程序通常称之为网络爬虫或蜘蛛程序。即便是在大数据时代，数据对于中小企业来说仍然是硬伤和短板，有些数据需要通过开放或付费的数据接口来获得，其他的行业数据和竞对数据则必须要通过网络数据采集的方式来获得。不管使用哪种方式获取网络数据资源，Python 语言都是非常好的选择，因为 Python 的">
<meta property="og:type" content="article">
<meta property="og:title" content="Python进阶（十）-- 网络爬虫">
<meta property="og:url" content="http://example.com/2022/11/14/Python%E8%BF%9B%E9%98%B6(%E5%8D%81)%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/index.html">
<meta property="og:site_name" content="HaHazhong&amp;Blog">
<meta property="og:description" content="用Python获取网络数据网络数据采集是 Python 语言非常擅长的领域，上节课我们讲到，实现网络数据采集的程序通常称之为网络爬虫或蜘蛛程序。即便是在大数据时代，数据对于中小企业来说仍然是硬伤和短板，有些数据需要通过开放或付费的数据接口来获得，其他的行业数据和竞对数据则必须要通过网络数据采集的方式来获得。不管使用哪种方式获取网络数据资源，Python 语言都是非常好的选择，因为 Python 的">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-11-14T13:23:31.000Z">
<meta property="article:modified_time" content="2022-11-17T13:44:41.870Z">
<meta property="article:author" content="hahazhong">
<meta property="article:tag" content="python">
<meta property="article:tag" content="网络爬虫">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2022/11/14/Python%E8%BF%9B%E9%98%B6(%E5%8D%81)%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Python进阶（十）-- 网络爬虫 | HaHazhong&Blog</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?7c47c023a1e4dabebf67e119f4b6453d";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="HaHazhong&Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">HaHazhong&Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">人生苦短，我用Python</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags name fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/11/14/Python%E8%BF%9B%E9%98%B6(%E5%8D%81)%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="hahazhong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HaHazhong&Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Python进阶（十）-- 网络爬虫
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-11-14 21:23:31" itemprop="dateCreated datePublished" datetime="2022-11-14T21:23:31+08:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-17 21:44:41" itemprop="dateModified" datetime="2022-11-17T21:44:41+08:00">2022-11-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%BF%9B%E9%98%B6/" itemprop="url" rel="index"><span itemprop="name">进阶</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/11/14/Python%E8%BF%9B%E9%98%B6(%E5%8D%81)%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/Python%E8%BF%9B%E9%98%B6(%E5%8D%81)%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="用Python获取网络数据"><a href="#用Python获取网络数据" class="headerlink" title="用Python获取网络数据"></a>用Python获取网络数据</h1><p>网络数据采集是 Python 语言非常擅长的领域，上节课我们讲到，实现网络数据采集的程序通常称之为网络爬虫或蜘蛛程序。即便是在大数据时代，数据对于中小企业来说仍然是硬伤和短板，有些数据需要通过开放或付费的数据接口来获得，其他的行业数据和竞对数据则必须要通过网络数据采集的方式来获得。不管使用哪种方式获取网络数据资源，Python 语言都是非常好的选择，因为 Python 的标准库和三方库都对网络数据采集提供了良好的支持。</p>
<span id="more"></span>  

<h2 id="requests库"><a href="#requests库" class="headerlink" title="requests库"></a>requests库</h2><p>要使用 Python 获取网络数据，我们推荐大家使用名为<code>requests</code> 的三方库，这个库我们在之前的课程中其实已经使用过了。按照官方网站的解释，<code>requests</code>是基于 Python 标准库进行了封装，简化了通过 HTTP 或 HTTPS 访问网络资源的操作。上课我们提到过，HTTP 是一个请求响应式的协议，当我们在浏览器中输入正确的 <a target="_blank" rel="noopener" href="https://developer.mozilla.org/zh-CN/docs/Learn/Common_questions/What_is_a_URL">URL</a>（通常也称为网址）并按下 Enter 键时，我们就向网络上的 <a target="_blank" rel="noopener" href="https://developer.mozilla.org/zh-CN/docs/Learn/Common_questions/What_is_a_web_server">Web 服务器</a>发送了一个 HTTP 请求，服务器在收到请求后会给我们一个 HTTP 响应。在 Chrome 浏览器中的菜单中打开“开发者工具”切换到“Network”选项卡就能够查看 HTTP 请求和响应到底是什么样子的，如下图所示。</p>
<p>通过<code>requests</code>库，我们可以让 Python 程序向浏览器一样向 Web 服务器发起请求，并接收服务器返回的响应，从响应中我们就可以提取出想要的数据。浏览器呈现给我们的网页是用 <a target="_blank" rel="noopener" href="https://developer.mozilla.org/zh-CN/docs/Web/HTML">HTML</a> 编写的，浏览器相当于是 HTML 的解释器环境，我们看到的网页中的内容都包含在 HTML 的标签中。在获取到 HTML 代码后，就可以从标签的属性或标签体中提取内容。下面例子演示了如何获取网页 HTML 代码，我们通过<code>requests</code>库的<code>get</code>函数，获取了搜狐首页的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">resp = requests.get(<span class="string">&#x27;https://www.sohu.com/&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> resp.status_code == <span class="number">200</span>:</span><br><span class="line">    <span class="built_in">print</span>(resp.text)</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>说明</strong>：上面代码中的变量<code>resp</code>是一个<code>Response</code>对象（<code>requests</code>库封装的类型），通过该对象的<code>status_code</code>属性可以获取响应状态码，而该对象的<code>text</code>属性可以帮我们获取到页面的 HTML 代码。</p>
</blockquote>
<p>由于<code>Response</code>对象的<code>text</code>是一个字符串，所以我们可以利用之前讲过的正则表达式的知识，从页面的 HTML 代码中提取新闻的标题和链接，代码如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;&lt;a.*?href=&quot;(.*?)&quot;.*?title=&quot;(.*?)&quot;.*?&gt;&#x27;</span>)</span><br><span class="line">resp = requests.get(<span class="string">&#x27;https://www.sohu.com/&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> resp.status_code == <span class="number">200</span>:</span><br><span class="line">    all_matches = pattern.findall(resp.text)</span><br><span class="line">    <span class="keyword">for</span> href, title <span class="keyword">in</span> all_matches:</span><br><span class="line">        <span class="built_in">print</span>(href)</span><br><span class="line">        <span class="built_in">print</span>(title)</span><br></pre></td></tr></table></figure>

<p>除了文本内容，我们也可以使用<code>requests</code>库通过 URL 获取二进制资源。下面的例子演示了如何获取百度 Logo 并保存到名为<code>baidu.png</code>的本地文件中。可以在百度的首页上右键点击百度Logo，并通过“复制图片地址”菜单项获取图片的 URL。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">resp = requests.get(<span class="string">&#x27;https://www.baidu.com/img/PCtm_d9c8750bed0b3c7d089fa7d55720d6cf.png&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;baidu.png&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    file.write(resp.content)</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>说明</strong>：<code>Response</code>对象的<code>content</code>属性可以获得服务器响应的二进制数据。</p>
</blockquote>
<p><code>requests</code>库非常好用而且功能上也比较强大和完整，具体的内容我们在使用的过程中为大家一点点剖析。想解锁关于<code>requests</code>库更多的知识，可以阅读它的<a target="_blank" rel="noopener" href="https://docs.python-requests.org/zh_CN/latest/">官方文档</a>。</p>
<h2 id="编写爬虫代码"><a href="#编写爬虫代码" class="headerlink" title="编写爬虫代码"></a>编写爬虫代码</h2><p>接下来，我们以“豆瓣电影”为例，为大家讲解如何编写爬虫代码。按照上面提供的方法，我们先使用<code>requests</code>获取到网页的HTML代码，然后将整个代码看成一个长字符串，这样我们就可以使用正则表达式的捕获组从字符串提取我们需要的内容。下面的代码演示了如何从<a target="_blank" rel="noopener" href="https://movie.douban.com/">豆瓣电影</a>获取排前250名的电影的名称。<a target="_blank" rel="noopener" href="https://movie.douban.com/top250">豆瓣电影Top250</a>的页面结构和对应代码如下图所示，可以看出，每页共展示了25部电影，如果要获取到 Top250 数据，我们共需要访问10个页面，对应的地址是<a target="_blank" rel="noopener" href="https://movie.douban.com/top250?start=xxx">https://movie.douban.com/top250?start=xxx</a>，这里的<code>xxx</code>如果为<code>0</code>就是第一页，如果<code>xxx</code>的值是<code>100</code>，那么我们可以访问到第五页。为了代码简单易读，我们只获取电影的标题和评分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">    resp = requests.get(</span><br><span class="line">        url=<span class="string">f&#x27;https://movie.douban.com/top250?start=<span class="subst">&#123;(page - <span class="number">1</span>) * <span class="number">25</span>&#125;</span>&#x27;</span>,</span><br><span class="line">        <span class="comment"># 如果不设置HTTP请求头中的User-Agent，豆瓣会检测出不是浏览器而阻止我们的请求。</span></span><br><span class="line">        <span class="comment"># 通过get函数的headers参数设置User-Agent的值，具体的值可以在浏览器的开发者工具查看到。</span></span><br><span class="line">        <span class="comment"># 用爬虫访问大部分网站时，将爬虫伪装成来自浏览器的请求都是非常重要的一步。</span></span><br><span class="line">        headers=&#123;<span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36&#x27;</span>&#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 通过正则表达式获取class属性为title且标签体不以&amp;开头的span标签并用捕获组提取标签内容</span></span><br><span class="line">    pattern1 = re.<span class="built_in">compile</span>(<span class="string">r&#x27;&lt;span class=&quot;title&quot;&gt;([^&amp;]*?)&lt;/span&gt;&#x27;</span>)</span><br><span class="line">    titles = pattern1.findall(resp.text)</span><br><span class="line">    <span class="comment"># 通过正则表达式获取class属性为rating_num的span标签并用捕获组提取标签内容</span></span><br><span class="line">    pattern2 = re.<span class="built_in">compile</span>(<span class="string">r&#x27;&lt;span class=&quot;rating_num&quot;.*?&gt;(.*?)&lt;/span&gt;&#x27;</span>)</span><br><span class="line">    ranks = pattern2.findall(resp.text)</span><br><span class="line">    <span class="comment"># 使用zip压缩两个列表，循环遍历所有的电影标题和评分</span></span><br><span class="line">    <span class="keyword">for</span> title, rank <span class="keyword">in</span> <span class="built_in">zip</span>(titles, ranks):</span><br><span class="line">        <span class="built_in">print</span>(title, rank)</span><br><span class="line">    <span class="comment"># 随机休眠1-5秒，避免爬取页面过于频繁</span></span><br><span class="line">    time.sleep(random.random() * <span class="number">4</span> + <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>说明</strong>：通过分析豆瓣网的robots协议，我们发现豆瓣网并不拒绝百度爬虫获取它的数据，因此我们也可以将爬虫伪装成百度的爬虫，将<code>get</code>函数的<code>headers</code>参数修改为：<code>headers=&#123;&#39;User-Agent&#39;: &#39;BaiduSpider&#39;&#125;</code>。</p>
</blockquote>
<h2 id="使用-IP-代理"><a href="#使用-IP-代理" class="headerlink" title="使用 IP 代理"></a>使用 IP 代理</h2><p>让爬虫程序隐匿自己的身份对编写爬虫程序来说是比较重要的，很多网站对爬虫都比较反感的，因为爬虫会耗费掉它们很多的网络带宽并制造很多无效的流量。要隐匿身份通常需要使用<strong>商业 IP 代理</strong>（如蘑菇代理、芝麻代理、快代理等），让被爬取的网站无法获取爬虫程序来源的真实 IP 地址，也就无法简单的通过 IP 地址对爬虫程序进行封禁。</p>
<p>下面以<a target="_blank" rel="noopener" href="http://www.moguproxy.com/">蘑菇代理</a>为例，为大家讲解商业 IP 代理的使用方法。首先需要在该网站注册一个账号，注册账号后就可以<a target="_blank" rel="noopener" href="http://www.moguproxy.com/buy">购买</a>相应的套餐来获得商业 IP 代理。作为商业用途，建议大家购买不限量套餐，这样可以根据实际需要获取足够多的代理 IP 地址；作为学习用途，可以购买包时套餐或根据自己的需求来决定。蘑菇代理提供了两种接入代理的方式，分别是 API 私密代理和 HTTP 隧道代理，前者是通过请求蘑菇代理的 API 接口获取代理服务器地址，后者是直接使用统一的入口（蘑菇代理提供的域名）进行接入。</p>
<p>下面，我们以HTTP隧道代理为例，为大家讲解接入 IP 代理的方式，大家也可以直接参考蘑菇代理官网提供的代码来为爬虫设置代理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">APP_KEY = <span class="string">&#x27;Wnp******************************XFx&#x27;</span></span><br><span class="line">PROXY_HOST = <span class="string">&#x27;secondtransfer.moguproxy.com:9001&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">    resp = requests.get(</span><br><span class="line">        url=<span class="string">f&#x27;https://movie.douban.com/top250?start=<span class="subst">&#123;(page - <span class="number">1</span>) * <span class="number">25</span>&#125;</span>&#x27;</span>,</span><br><span class="line">        <span class="comment"># 需要在HTTP请求头设置代理的身份认证方式</span></span><br><span class="line">        headers=&#123;</span><br><span class="line">            <span class="string">&#x27;Proxy-Authorization&#x27;</span>: <span class="string">f&#x27;Basic <span class="subst">&#123;APP_KEY&#125;</span>&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Accept-Language&#x27;</span>: <span class="string">&#x27;zh-CN,zh;q=0.8,en-US;q=0.6,en;q=0.4&#x27;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="comment"># 设置代理服务器</span></span><br><span class="line">        proxies=&#123;</span><br><span class="line">            <span class="string">&#x27;http&#x27;</span>: <span class="string">f&#x27;http://<span class="subst">&#123;PROXY_HOST&#125;</span>&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;https&#x27;</span>: <span class="string">f&#x27;https://<span class="subst">&#123;PROXY_HOST&#125;</span>&#x27;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        verify=<span class="literal">False</span></span><br><span class="line">    )</span><br><span class="line">    pattern1 = re.<span class="built_in">compile</span>(<span class="string">r&#x27;&lt;span class=&quot;title&quot;&gt;([^&amp;]*?)&lt;/span&gt;&#x27;</span>)</span><br><span class="line">    titles = pattern1.findall(resp.text)</span><br><span class="line">    pattern2 = re.<span class="built_in">compile</span>(<span class="string">r&#x27;&lt;span class=&quot;rating_num&quot;.*?&gt;(.*?)&lt;/span&gt;&#x27;</span>)</span><br><span class="line">    ranks = pattern2.findall(resp.text)</span><br><span class="line">    <span class="keyword">for</span> title, rank <span class="keyword">in</span> <span class="built_in">zip</span>(titles, ranks):</span><br><span class="line">        <span class="built_in">print</span>(title, rank)</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>说明</strong>：上面的代码需要修改<code>APP_KEY</code>为自己创建的订单对应的<code>Appkey</code>值，这个值可以在用户中心用户订单中查看到。蘑菇代理提供了免费的 API 代理和 HTTP 隧道代理试用，但是试用的代理接通率不能保证，建议大家还是直接购买一个在自己支付能力范围内的代理服务来体验。</p>
<p><strong>另注</strong>：蘑菇代理目前已经停止服务了，大家可以按照上面讲解的方式使用其他商业代理即可。</p>
</blockquote>
<h1 id="用Python解析HTML页面"><a href="#用Python解析HTML页面" class="headerlink" title="用Python解析HTML页面"></a>用Python解析HTML页面</h1><p>在前面的课程中，我们讲到了使用<code>request</code>三方库获取网络资源，还介绍了一些前端的基础知识。接下来，我们继续探索如何解析 HTML 代码，从页面中提取出有用的信息。之前，我们尝试过用正则表达式的捕获组操作提取页面内容，但是写出一个正确的正则表达式也是一件让人头疼的事情。为了解决这个问题，我们得先深入的了解一下 HTML 页面的结构，并在此基础上研究另外的解析页面的方法。</p>
<h2 id="HTML-页面的结构"><a href="#HTML-页面的结构" class="headerlink" title="HTML 页面的结构"></a>HTML 页面的结构</h2><p>我们在浏览器中打开任意一个网站，然后通过鼠标右键菜单，选择“显示网页源代码”菜单项，就可以看到网页对应的 HTML 代码。</p>
<p>代码的第<code>1</code>行是文档类型声明，第<code>2</code>行的<code>&lt;html&gt;</code>标签是整个页面根标签的开始标签，最后一行是根标签的结束标签<code>&lt;/html&gt;</code>。<code>&lt;html&gt;</code>标签下面有两个子标签<code>&lt;head&gt;</code>和<code>&lt;body&gt;</code>，放在<code>&lt;body&gt;</code>标签下的内容会显示在浏览器窗口中，这部分内容是网页的主体；放在<code>&lt;head&gt;</code>标签下的内容不会显示在浏览器窗口中，但是却包含了页面重要的元信息，通常称之为网页的头部。HTML 页面大致的代码结构如下所示。</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!doctype <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 页面的元信息，如字符编码、标题、关键字、媒体查询等 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 页面的主体，显示在浏览器窗口中的内容 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>标签、层叠样式表（CSS）、JavaScript 是构成 HTML 页面的三要素，其中标签用来承载页面要显示的内容，CSS 负责对页面的渲染，而 JavaScript 用来控制页面的交互式行为。要实现 HTML 页面的解析，可以使用 XPath 的语法，它原本是 XML 的一种查询语法，可以根据 HTML 标签的层次结构提取标签中的内容或标签属性；此外，也可以使用 CSS 选择器来定位页面元素，就跟用 CSS 渲染页面元素是同样的道理。</p>
<h2 id="XPath-解析"><a href="#XPath-解析" class="headerlink" title="XPath 解析"></a>XPath 解析</h2><p>XPath 是在 XML（eXtensible Markup Language）文档中查找信息的一种语法，XML 跟 HTML 类似也是一种用标签承载数据的标签语言，不同之处在于 XML 的标签是可扩展的，可以自定义的，而且 XML 对语法有更严格的要求。XPath 使用路径表达式来选取 XML 文档中的节点或者节点集，这里所说的节点包括元素、属性、文本、命名空间、处理指令、注释、根节点等。下面我们通过一个例子来说明如何使用 XPath 对页面进行解析。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">bookstore</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">book</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">title</span> <span class="attr">lang</span>=<span class="string">&quot;eng&quot;</span>&gt;</span>Harry Potter<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">price</span>&gt;</span>29.99<span class="tag">&lt;/<span class="name">price</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">book</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">book</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">title</span> <span class="attr">lang</span>=<span class="string">&quot;zh&quot;</span>&gt;</span>Learning XML<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">price</span>&gt;</span>39.95<span class="tag">&lt;/<span class="name">price</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">book</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bookstore</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>对于上面的 XML 文件，我们可以用如下所示的 XPath 语法获取文档中的节点。</p>
<table>
<thead>
<tr>
<th>路径表达式</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td><code>/bookstore</code></td>
<td>选取根元素 bookstore。<strong>注意</strong>：假如路径起始于正斜杠( &#x2F; )，则此路径始终代表到某元素的绝对路径！</td>
</tr>
<tr>
<td><code>//book</code></td>
<td>选取所有 book 子元素，而不管它们在文档中的位置。</td>
</tr>
<tr>
<td><code>//@lang</code></td>
<td>选取名为 lang 的所有属性。</td>
</tr>
<tr>
<td><code>/bookstore/book[1]</code></td>
<td>选取属于 bookstore 子元素的第一个 book 元素。</td>
</tr>
<tr>
<td><code>/bookstore/book[last()]</code></td>
<td>选取属于 bookstore 子元素的最后一个 book 元素。</td>
</tr>
<tr>
<td><code>/bookstore/book[last()-1]</code></td>
<td>选取属于 bookstore 子元素的倒数第二个 book 元素。</td>
</tr>
<tr>
<td><code>/bookstore/book[position()&lt;3]</code></td>
<td>选取最前面的两个属于 bookstore 元素的子元素的 book 元素。</td>
</tr>
<tr>
<td><code>//title[@lang]</code></td>
<td>选取所有拥有名为 lang 的属性的 title 元素。</td>
</tr>
<tr>
<td><code>//title[@lang=&#39;eng&#39;]</code></td>
<td>选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。</td>
</tr>
<tr>
<td><code>/bookstore/book[price&gt;35.00]</code></td>
<td>选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。</td>
</tr>
<tr>
<td><code>/bookstore/book[price&gt;35.00]/title</code></td>
<td>选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。</td>
</tr>
</tbody></table>
<p>XPath还支持通配符用法，如下所示。</p>
<table>
<thead>
<tr>
<th>路径表达式</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td><code>/bookstore/*</code></td>
<td>选取 bookstore 元素的所有子元素。</td>
</tr>
<tr>
<td><code>//*</code></td>
<td>选取文档中的所有元素。</td>
</tr>
<tr>
<td><code>//title[@*]</code></td>
<td>选取所有带有属性的 title 元素。</td>
</tr>
</tbody></table>
<p>如果要选取多个节点，可以使用如下所示的方法。</p>
<table>
<thead>
<tr>
<th>路径表达式</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td><code>//book/title | //book/price</code></td>
<td>选取 book 元素的所有 title 和 price 元素。</td>
</tr>
<tr>
<td><code>//title | //price</code></td>
<td>选取文档中的所有 title 和 price 元素。</td>
</tr>
<tr>
<td><code>/bookstore/book/title | //price</code></td>
<td>选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>说明</strong>：上面的例子来自于“菜鸟教程”网站上的 <a target="_blank" rel="noopener" href="https://www.runoob.com/xpath/xpath-tutorial.html">XPath 教程</a>，有兴趣的读者可以自行阅读原文。</p>
</blockquote>
<p>当然，如果不理解或不熟悉 XPath 语法，可以在浏览器的开发者工具中按照如下所示的方法查看元素的 XPath 语法</p>
<p>实现 XPath 解析需要三方库<code>lxml</code> 的支持，可以使用下面的命令安装<code>lxml</code>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install lxml</span><br></pre></td></tr></table></figure>

<p>下面我们用 XPath 解析方式改写之前获取豆瓣电影 Top250的代码，如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">    resp = requests.get(</span><br><span class="line">        url=<span class="string">f&#x27;https://movie.douban.com/top250?start=<span class="subst">&#123;(page - <span class="number">1</span>) * <span class="number">25</span>&#125;</span>&#x27;</span>,</span><br><span class="line">        headers=&#123;<span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;BaiduSpider&#x27;</span>&#125;</span><br><span class="line">    )</span><br><span class="line">    tree = etree.HTML(resp.text)</span><br><span class="line">    <span class="comment"># 通过XPath语法从页面中提取电影标题</span></span><br><span class="line">    title_spans = tree.xpath(<span class="string">&#x27;//*[@id=&quot;content&quot;]/div/div[1]/ol/li/div/div[2]/div[1]/a/span[1]&#x27;</span>)</span><br><span class="line">    <span class="comment"># 通过XPath语法从页面中提取电影评分</span></span><br><span class="line">    rank_spans = tree.xpath(<span class="string">&#x27;//*[@id=&quot;content&quot;]/div/div[1]/ol/li[1]/div/div[2]/div[2]/div/span[2]&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> title_span, rank_span <span class="keyword">in</span> <span class="built_in">zip</span>(title_spans, rank_spans):</span><br><span class="line">        <span class="built_in">print</span>(title_span.text, rank_span.text)</span><br></pre></td></tr></table></figure>

<h2 id="CSS-选择器解析"><a href="#CSS-选择器解析" class="headerlink" title="CSS 选择器解析"></a>CSS 选择器解析</h2><p>对于熟悉 CSS 选择器和 JavaScript 的开发者来说，通过 CSS 选择器获取页面元素可能是更为简单的选择，因为浏览器中运行的 JavaScript 本身就可以<code>document</code>对象的<code>querySelector()</code>和<code>querySelectorAll()</code>方法基于 CSS 选择器获取页面元素。在 Python 中，我们可以利用三方库<code>beautifulsoup4</code>或<code>pyquery</code>来做同样的事情。Beautiful Soup 可以用来解析 HTML 和 XML 文档，修复含有未闭合标签等错误的文档，通过为待解析的页面在内存中创建一棵树结构，实现对从页面中提取数据操作的封装。可以用下面的命令来安装 Beautiful Soup。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install beautifulsoup4</span><br></pre></td></tr></table></figure>

<p>下面是使用<code>bs4</code>改写的获取豆瓣电影Top250电影名称的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> bs4</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">    resp = requests.get(</span><br><span class="line">        url=<span class="string">f&#x27;https://movie.douban.com/top250?start=<span class="subst">&#123;(page - <span class="number">1</span>) * <span class="number">25</span>&#125;</span>&#x27;</span>,</span><br><span class="line">        headers=&#123;<span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;BaiduSpider&#x27;</span>&#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 创建BeautifulSoup对象</span></span><br><span class="line">    soup = bs4.BeautifulSoup(resp.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">    <span class="comment"># 通过CSS选择器从页面中提取包含电影标题的span标签</span></span><br><span class="line">    title_spans = soup.select(<span class="string">&#x27;div.info &gt; div.hd &gt; a &gt; span:nth-child(1)&#x27;</span>)</span><br><span class="line">    <span class="comment"># 通过CSS选择器从页面中提取包含电影评分的span标签</span></span><br><span class="line">    rank_spans = soup.select(<span class="string">&#x27;div.info &gt; div.bd &gt; div &gt; span.rating_num&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> title_span, rank_span <span class="keyword">in</span> <span class="built_in">zip</span>(title_spans, rank_spans):</span><br><span class="line">        <span class="built_in">print</span>(title_span.text, rank_span.text)</span><br></pre></td></tr></table></figure>

<p>关于 BeautifulSoup 更多的知识，可以参考它的<a target="_blank" rel="noopener" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/">官方文档</a>。</p>
<h3 id="简单的总结"><a href="#简单的总结" class="headerlink" title="简单的总结"></a>简单的总结</h3><p>下面我们对三种解析方式做一个简单比较。</p>
<table>
<thead>
<tr>
<th>解析方式</th>
<th>对应的模块</th>
<th>速度</th>
<th>使用难度</th>
</tr>
</thead>
<tbody><tr>
<td>正则表达式解析</td>
<td><code>re</code></td>
<td>快</td>
<td>困难</td>
</tr>
<tr>
<td>XPath 解析</td>
<td><code>lxml</code></td>
<td>快</td>
<td>一般</td>
</tr>
<tr>
<td>CSS 选择器解析</td>
<td><code>bs4</code>或<code>pyquery</code></td>
<td>不确定</td>
<td>简单</td>
</tr>
</tbody></table>
<h1 id="爬虫框架Scrapy简介"><a href="#爬虫框架Scrapy简介" class="headerlink" title="爬虫框架Scrapy简介"></a>爬虫框架Scrapy简介</h1><p>当你写了很多个爬虫程序之后，你会发现每次写爬虫程序时，都需要将页面获取、页面解析、爬虫调度、异常处理、反爬应对这些代码从头至尾实现一遍，这里面有很多工作其实都是简单乏味的重复劳动。那么，有没有什么办法可以提升我们编写爬虫代码的效率呢？答案是肯定的，那就是利用爬虫框架，而在所有的爬虫框架中，Scrapy 应该是最流行、最强大的框架。</p>
<h2 id="Scrapy-概述"><a href="#Scrapy-概述" class="headerlink" title="Scrapy 概述"></a>Scrapy 概述</h2><p>Scrapy 是基于 Python 的一个非常流行的网络爬虫框架，可以用来抓取 Web 站点并从页面中提取结构化的数据。</p>
<h3 id="Scrapy的组件"><a href="#Scrapy的组件" class="headerlink" title="Scrapy的组件"></a>Scrapy的组件</h3><p>我们先来说说 Scrapy 中的组件。</p>
<ol>
<li>Scrapy 引擎（Engine）：用来控制整个系统的数据处理流程。</li>
<li>调度器（Scheduler）：调度器从引擎接受请求并排序列入队列，并在引擎发出请求后返还给它们。</li>
<li>下载器（Downloader）：下载器的主要职责是抓取网页并将网页内容返还给蜘蛛（Spiders）。</li>
<li>蜘蛛程序（Spiders）：蜘蛛是用户自定义的用来解析网页并抓取特定URL的类，每个蜘蛛都能处理一个域名或一组域名，简单的说就是用来定义特定网站的抓取和解析规则的模块。</li>
<li>数据管道（Item Pipeline）：管道的主要责任是负责处理有蜘蛛从网页中抽取的数据条目，它的主要任务是清理、验证和存储数据。当页面被蜘蛛解析后，将被发送到数据管道，并经过几个特定的次序处理数据。每个数据管道组件都是一个 Python 类，它们获取了数据条目并执行对数据条目进行处理的方法，同时还需要确定是否需要在数据管道中继续执行下一步或是直接丢弃掉不处理。数据管道通常执行的任务有：清理 HTML 数据、验证解析到的数据（检查条目是否包含必要的字段）、检查是不是重复数据（如果重复就丢弃）、将解析到的数据存储到数据库（关系型数据库或 NoSQL 数据库）中。</li>
<li>中间件（Middlewares）：中间件是介于引擎和其他组件之间的一个钩子框架，主要是为了提供自定义的代码来拓展 Scrapy 的功能，包括下载器中间件和蜘蛛中间件。</li>
</ol>
<h3 id="数据处理流程"><a href="#数据处理流程" class="headerlink" title="数据处理流程"></a>数据处理流程</h3><p>Scrapy 的整个数据处理流程由引擎进行控制，通常的运转流程包括以下的步骤：</p>
<ol>
<li><p>引擎询问蜘蛛需要处理哪个网站，并让蜘蛛将第一个需要处理的 URL 交给它。</p>
</li>
<li><p>引擎让调度器将需要处理的 URL 放在队列中。</p>
</li>
<li><p>引擎从调度那获取接下来进行爬取的页面。</p>
</li>
<li><p>调度将下一个爬取的 URL 返回给引擎，引擎将它通过下载中间件发送到下载器。</p>
</li>
<li><p>当网页被下载器下载完成以后，响应内容通过下载中间件被发送到引擎；如果下载失败了，引擎会通知调度器记录这个 URL，待会再重新下载。</p>
</li>
<li><p>引擎收到下载器的响应并将它通过蜘蛛中间件发送到蜘蛛进行处理。</p>
</li>
<li><p>蜘蛛处理响应并返回爬取到的数据条目，此外还要将需要跟进的新的 URL 发送给引擎。</p>
</li>
<li><p>引擎将抓取到的数据条目送入数据管道，把新的 URL 发送给调度器放入队列中。</p>
</li>
</ol>
<p>上述操作中的第2步到第8步会一直重复直到调度器中没有需要请求的 URL，爬虫就停止工作。</p>
<h2 id="安装和使用Scrapy"><a href="#安装和使用Scrapy" class="headerlink" title="安装和使用Scrapy"></a>安装和使用Scrapy</h2><p>可以使用 Python 的包管理工具<code>pip</code>来安装 Scrapy。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br></pre></td></tr></table></figure>

<p>在命令行中使用<code>scrapy</code>命令创建名为<code>demo</code>的项目。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject demo</span><br></pre></td></tr></table></figure>

<p>项目的目录结构如下图所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">demo</span><br><span class="line">|____ demo</span><br><span class="line">|________ spiders</span><br><span class="line">|____________ __init__.py</span><br><span class="line">|________ __init__.py</span><br><span class="line">|________ items.py</span><br><span class="line">|________ middlewares.py</span><br><span class="line">|________ pipelines.py</span><br><span class="line">|________ settings.py</span><br><span class="line">|____ scrapy.cfg</span><br></pre></td></tr></table></figure>

<p>切换到<code>demo</code> 目录，用下面的命令创建名为<code>douban</code>的蜘蛛程序。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider douban movie.douban.com</span><br></pre></td></tr></table></figure>

<h3 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><p>接下来，我们实现一个爬取豆瓣电影 Top250 电影标题、评分和金句的爬虫。</p>
<ol>
<li><p>在<code>items.py</code>的<code>Item</code>类中定义字段，这些字段用来保存数据，方便后续的操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DoubanItem</span>(scrapy.Item):</span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    score = scrapy.Field()</span><br><span class="line">    motto = scrapy.Field()</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改<code>spiders</code>文件夹中名为<code>douban.py</code> 的文件，它是蜘蛛程序的核心，需要我们添加解析页面的代码。在这里，我们可以通过对<code>Response</code>对象的解析，获取电影的信息，代码如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Selector, Request</span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> HtmlResponse</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> demo.items <span class="keyword">import</span> MovieItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DoubanSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;douban&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;movie.douban.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://movie.douban.com/top250?start=0&amp;filter=&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response: HtmlResponse</span>):</span><br><span class="line">        sel = Selector(response)</span><br><span class="line">        movie_items = sel.css(<span class="string">&#x27;#content &gt; div &gt; div.article &gt; ol &gt; li&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> movie_sel <span class="keyword">in</span> movie_items:</span><br><span class="line">            item = MovieItem()</span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = movie_sel.css(<span class="string">&#x27;.title::text&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;score&#x27;</span>] = movie_sel.css(<span class="string">&#x27;.rating_num::text&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;motto&#x27;</span>] = movie_sel.css(<span class="string">&#x27;.inq::text&#x27;</span>).extract_first()</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>
<p>通过上面的代码不难看出，我们可以使用 CSS 选择器进行页面解析。当然，如果你愿意也可以使用 XPath 或正则表达式进行页面解析，对应的方法分别是<code>xpath</code>和<code>re</code>。</p>
<p>如果还要生成后续爬取的请求，我们可以用<code>yield</code>产出<code>Request</code>对象。<code>Request</code>对象有两个非常重要的属性，一个是<code>url</code>，它代表了要请求的地址；一个是<code>callback</code>，它代表了获得响应之后要执行的回调函数。我们可以将上面的代码稍作修改。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Selector, Request</span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> HtmlResponse</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> demo.items <span class="keyword">import</span> MovieItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DoubanSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;douban&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;movie.douban.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://movie.douban.com/top250?start=0&amp;filter=&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response: HtmlResponse</span>):</span><br><span class="line">        sel = Selector(response)</span><br><span class="line">        movie_items = sel.css(<span class="string">&#x27;#content &gt; div &gt; div.article &gt; ol &gt; li&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> movie_sel <span class="keyword">in</span> movie_items:</span><br><span class="line">            item = MovieItem()</span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = movie_sel.css(<span class="string">&#x27;.title::text&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;score&#x27;</span>] = movie_sel.css(<span class="string">&#x27;.rating_num::text&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;motto&#x27;</span>] = movie_sel.css(<span class="string">&#x27;.inq::text&#x27;</span>).extract_first()</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">        hrefs = sel.css(<span class="string">&#x27;#content &gt; div &gt; div.article &gt; div.paginator &gt; a::attr(&quot;href&quot;)&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> href <span class="keyword">in</span> hrefs:</span><br><span class="line">            full_url = response.urljoin(href.extract())</span><br><span class="line">            <span class="keyword">yield</span> Request(url=full_url)</span><br></pre></td></tr></table></figure>

<p>到这里，我们已经可以通过下面的命令让爬虫运转起来。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl movie</span><br></pre></td></tr></table></figure>

<p>可以在控制台看到爬取到的数据，如果想将这些数据保存到文件中，可以通过<code>-o</code>参数来指定文件名，Scrapy 支持我们将爬取到的数据导出成 JSON、CSV、XML 等格式。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl moive -o result.json</span><br></pre></td></tr></table></figure>

<p>不知大家是否注意到，通过运行爬虫获得的 JSON 文件中有<code>275</code>条数据，那是因为首页被重复爬取了。要解决这个问题，可以对上面的代码稍作调整，不在<code>parse</code>方法中解析获取新页面的 URL，而是通过<code>start_requests</code>方法提前准备好待爬取页面的 URL，调整后的代码如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Selector, Request</span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> HtmlResponse</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> demo.items <span class="keyword">import</span> MovieItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DoubanSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;douban&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;movie.douban.com&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">            <span class="keyword">yield</span> Request(url=<span class="string">f&#x27;https://movie.douban.com/top250?start=<span class="subst">&#123;page * <span class="number">25</span>&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response: HtmlResponse</span>):</span><br><span class="line">        sel = Selector(response)</span><br><span class="line">        movie_items = sel.css(<span class="string">&#x27;#content &gt; div &gt; div.article &gt; ol &gt; li&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> movie_sel <span class="keyword">in</span> movie_items:</span><br><span class="line">            item = MovieItem()</span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = movie_sel.css(<span class="string">&#x27;.title::text&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;score&#x27;</span>] = movie_sel.css(<span class="string">&#x27;.rating_num::text&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;motto&#x27;</span>] = movie_sel.css(<span class="string">&#x27;.inq::text&#x27;</span>).extract_first()</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果希望完成爬虫数据的持久化，可以在数据管道中处理蜘蛛程序产生的<code>Item</code>对象。例如，我们可以通过前面讲到的<code>openpyxl</code>操作 Excel 文件，将数据写入 Excel 文件中，代码如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openpyxl</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> demo.items <span class="keyword">import</span> MovieItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MovieItemPipeline</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.wb = openpyxl.Workbook()</span><br><span class="line">        self.sheet = self.wb.active</span><br><span class="line">        self.sheet.title = <span class="string">&#x27;Top250&#x27;</span></span><br><span class="line">        self.sheet.append((<span class="string">&#x27;名称&#x27;</span>, <span class="string">&#x27;评分&#x27;</span>, <span class="string">&#x27;名言&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item: MovieItem, spider</span>):</span><br><span class="line">        self.sheet.append((item[<span class="string">&#x27;title&#x27;</span>], item[<span class="string">&#x27;score&#x27;</span>], item[<span class="string">&#x27;motto&#x27;</span>]))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.wb.save(<span class="string">&#x27;豆瓣电影数据.xlsx&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>上面的<code>process_item</code>和<code>close_spider</code>都是回调方法（钩子函数）， 简单的说就是 Scrapy 框架会自动去调用的方法。当蜘蛛程序产生一个<code>Item</code>对象交给引擎时，引擎会将该<code>Item</code>对象交给数据管道，这时我们配置好的数据管道的<code>parse_item</code>方法就会被执行，所以我们可以在该方法中获取数据并完成数据的持久化操作。另一个方法<code>close_spider</code>是在爬虫结束运行前会自动执行的方法，在上面的代码中，我们在这个地方进行了保存 Excel 文件的操作，相信这段代码大家是很容易读懂的。</p>
<p>总而言之，数据管道可以帮助我们完成以下操作：</p>
<ul>
<li>清理 HTML 数据，验证爬取的数据。</li>
<li>丢弃重复的不必要的内容。</li>
<li>将爬取的结果进行持久化操作。</li>
</ul>
</li>
<li><p>修改<code>settings.py</code>文件对项目进行配置，主要需要修改以下几个配置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用户浏览器</span></span><br><span class="line">USER_AGENT = <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 并发请求数量 </span></span><br><span class="line">CONCURRENT_REQUESTS = <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载延迟</span></span><br><span class="line">DOWNLOAD_DELAY = <span class="number">3</span></span><br><span class="line"><span class="comment"># 随机化下载延迟</span></span><br><span class="line">RANDOMIZE_DOWNLOAD_DELAY = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否遵守爬虫协议</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置数据管道</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&#x27;demo.pipelines.MovieItemPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>说明</strong>：上面配置文件中的<code>ITEM_PIPELINES</code>选项是一个字典，可以配置多个处理数据的管道，后面的数字代表了执行的优先级，数字小的先执行。</p>
</blockquote>
</li>
</ol>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>hahazhong
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2022/11/14/Python%E8%BF%9B%E9%98%B6(%E5%8D%81)%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/" title="Python进阶（十）-- 网络爬虫">http://example.com/2022/11/14/Python进阶(十)网络爬虫/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a>
              <a href="/tags/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/" rel="tag"><i class="fa fa-tag"></i> 网络爬虫</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/01/Python%E8%BF%9B%E9%98%B6(%E4%B9%9D)%E6%93%8D%E4%BD%9Cexcel%E6%96%87%E4%BB%B6/" rel="prev" title="Python进阶（九）-- 操作excel文件">
      <i class="fa fa-chevron-left"></i> Python进阶（九）-- 操作excel文件
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/11/17/Python%E8%BF%9B%E9%98%B6(%E5%8D%81%E4%B8%80)Numpy%E6%A8%A1%E5%9D%97/" rel="next" title="Python进阶（十一）-- NumPy模块">
      Python进阶（十一）-- NumPy模块 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>


      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%94%A8Python%E8%8E%B7%E5%8F%96%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE"><span class="nav-number">1.</span> <span class="nav-text">用Python获取网络数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#requests%E5%BA%93"><span class="nav-number">1.1.</span> <span class="nav-text">requests库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E5%86%99%E7%88%AC%E8%99%AB%E4%BB%A3%E7%A0%81"><span class="nav-number">1.2.</span> <span class="nav-text">编写爬虫代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-IP-%E4%BB%A3%E7%90%86"><span class="nav-number">1.3.</span> <span class="nav-text">使用 IP 代理</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%94%A8Python%E8%A7%A3%E6%9E%90HTML%E9%A1%B5%E9%9D%A2"><span class="nav-number">2.</span> <span class="nav-text">用Python解析HTML页面</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#HTML-%E9%A1%B5%E9%9D%A2%E7%9A%84%E7%BB%93%E6%9E%84"><span class="nav-number">2.1.</span> <span class="nav-text">HTML 页面的结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XPath-%E8%A7%A3%E6%9E%90"><span class="nav-number">2.2.</span> <span class="nav-text">XPath 解析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CSS-%E9%80%89%E6%8B%A9%E5%99%A8%E8%A7%A3%E6%9E%90"><span class="nav-number">2.3.</span> <span class="nav-text">CSS 选择器解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E7%9A%84%E6%80%BB%E7%BB%93"><span class="nav-number">2.3.1.</span> <span class="nav-text">简单的总结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6Scrapy%E7%AE%80%E4%BB%8B"><span class="nav-number">3.</span> <span class="nav-text">爬虫框架Scrapy简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Scrapy-%E6%A6%82%E8%BF%B0"><span class="nav-number">3.1.</span> <span class="nav-text">Scrapy 概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Scrapy%E7%9A%84%E7%BB%84%E4%BB%B6"><span class="nav-number">3.1.1.</span> <span class="nav-text">Scrapy的组件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B"><span class="nav-number">3.1.2.</span> <span class="nav-text">数据处理流程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8Scrapy"><span class="nav-number">3.2.</span> <span class="nav-text">安装和使用Scrapy</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-number">3.2.1.</span> <span class="nav-text">一个简单的例子</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="hahazhong"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">hahazhong</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hahazhong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hahazhong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/zzt3344?spm=1010.2135.3001.5421" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;zzt3344?spm&#x3D;1010.2135.3001.5421" rel="noopener" target="_blank"><i class="crosshirs fa-fw"></i>CSDN</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://hahazhong.gitee.io/" title="Gitee → http:&#x2F;&#x2F;hahazhong.gitee.io&#x2F;" rel="noopener" target="_blank"><i class="fa-gitee fa-fw"></i>Gitee</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
 <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1808492017&auto=1&height=66"></iframe>
	 
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        


<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hahazhong</span>
<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_sphere.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qQysVh8YklEUcpV4v59e5tLb-gzGzoHsz',
      appKey     : 'yGnaYJoCNgNyERyHr1mboHC7',
      placeholder: "欢迎指教!",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
